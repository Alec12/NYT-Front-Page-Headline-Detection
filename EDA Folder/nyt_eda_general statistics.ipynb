{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from datetime import datetime\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\scj41343\\AppData\\Local\\Temp\\ipykernel_24960\\3231777788.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  rest = pd.read_csv(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392845, 35)\n",
      "(21917, 31)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "with gzip.open('rest_byline_cleaned.gz', 'rb') as f:\n",
    "    rest = pd.read_csv(f)\n",
    "print(rest.shape)\n",
    "\n",
    "with gzip.open('front_pages_byline_cleaned.gz', 'rb') as f:\n",
    "    front_pages = pd.read_csv(f)\n",
    "print(front_pages.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract only the articles\n",
    "rest = rest[rest['documenttype'] == 'article']\n",
    "front_pages = front_pages[front_pages['documenttype'] == 'article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing abstract for front pages and rest of the paper\n",
    "rabstractna = round(rest[\"abstract\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rabstractna)\n",
    "fpabstractna = round(front_pages[\"abstract\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fpabstractna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.52\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing snippet for front pages and rest of the paper\n",
    "rsnippetna = round(rest[\"snippet\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rsnippetna)\n",
    "fpsnippetna = round(front_pages[\"snippet\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fpsnippetna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing leadparagraph for front pages and rest of the paper\n",
    "rleadaparana = round(rest[\"leadparagraph\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rleadaparana)\n",
    "fpleadaparana = round(front_pages[\"leadparagraph\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fpleadaparana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.36\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing newsdesk for front pages and rest of the paper\n",
    "rnewsdeskna = round(rest[\"newsdesk\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rnewsdeskna)\n",
    "fpnewsdeskna = round(front_pages[\"newsdesk\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fpnewsdeskna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.35\n",
      "50.35\n"
     ]
    }
   ],
   "source": [
    "#we know that front page is section A page 1, so determine what percentage of the rest of the paper is missing print section and print page\n",
    "rprintsecna = round(rest[\"printsection\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rprintsecna)\n",
    "rprintpagena = round(rest[\"printpage\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rprintpagena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.8\n",
      "2.39\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing subsectionname for front pages and rest of the paper\n",
    "rsubsecnamena = round(rest[\"subsectionname\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rsubsecnamena)\n",
    "fpsubsecnamena = round(front_pages[\"subsectionname\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fpsubsecnamena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing publication dtate for front pages and rest of the paper\n",
    "rpubdatena = round(rest[\"pubdate\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rpubdatena)\n",
    "fppubdatena = round(front_pages[\"pubdate\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fppubdatena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#determine percentages of missing type of material for front pages and rest of the paper\n",
    "rtypena = round(rest[\"typeofmaterial\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(rtypena)\n",
    "fptypena = round(front_pages[\"typeofmaterial\"].isna().sum()*100/rest.shape[0],2)\n",
    "print(fptypena)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.16070039545473\n",
      "1\n",
      "250\n",
      "157.28011371452152\n",
      "3\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "#detetrmine min, max, and mean character lengths for snippets\n",
    "print(rest[\"snippet\"].astype(str).apply(len).mean())\n",
    "print(rest[\"snippet\"].astype(str).apply(len).min())\n",
    "print(rest[\"snippet\"].astype(str).apply(len).max())\n",
    "print(front_pages[\"snippet\"].astype(str).apply(len).mean())\n",
    "print(front_pages[\"snippet\"].astype(str).apply(len).min())\n",
    "print(front_pages[\"snippet\"].astype(str).apply(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260.56615027279304\n",
      "1\n",
      "4204\n",
      "249.72410472740611\n",
      "1\n",
      "1020\n"
     ]
    }
   ],
   "source": [
    "#detetrmine min, max, and mean character lengths for lead paragraphs\n",
    "print(rest[\"leadparagraph\"].astype(str).apply(len).mean())\n",
    "print(rest[\"leadparagraph\"].astype(str).apply(len).min())\n",
    "print(rest[\"leadparagraph\"].astype(str).apply(len).max())\n",
    "print(front_pages[\"leadparagraph\"].astype(str).apply(len).mean())\n",
    "print(front_pages[\"leadparagraph\"].astype(str).apply(len).min())\n",
    "print(front_pages[\"leadparagraph\"].astype(str).apply(len).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The New York Times']\n",
      "['The New York Times']\n"
     ]
    }
   ],
   "source": [
    "#print unique values for source\n",
    "print(rest[\"source\"].unique())\n",
    "print(front_pages[\"source\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sports' nan 'Home' 'Letters' 'Business' 'National' 'Science' 'Culture'\n",
      " 'Metro' 'Magazine' 'Foreign' 'BookReview' 'Arts&Leisure' 'Travel'\n",
      " 'TCulture' 'TDesign' 'Weekend' 'Styles' 'Dining' 'Society' 'RealEstate'\n",
      " 'OpEd' 'TStyle' 'Automobiles' 'SundayBusiness' 'Editorial' 'NODESK'\n",
      " 'Media' 'Summary' 'EdLife' 'Energy' 'Fashion' 'Washington' 'Politics'\n",
      " 'TLiving' 'SundayReview' 'NewsDesk' 'Upshot' 'Obits' 'StateByState'\n",
      " 'Insider' 'Museums' 'NYTNow' 'Your Money' 'Dealbook' 'TTravel'\n",
      " 'Universal' 'Metropolitan' 'WEB' 'Express' 'Corrections' 'Games'\n",
      " 'Podcasts' 'Well' 'Learning' 'SpecialSections' 'Watching' 'Climate'\n",
      " 'Smarter Living' 'Books' 'Investigative' 'Test' 'NYTI' 'Video' 'Photo'\n",
      " 'Gender' 'Neediest' 'Parenting' 'Graphics' 'Live' 'AtHome' 'Espa√±ol'\n",
      " 'InteractiveNews' 'Headway' 'Chinese' 'Weather'\n",
      " 'Projects and Initiatives' 'DigitalNewsDesign' 'Election Analytics'\n",
      " 'Local Investigations']\n",
      "['National' 'Foreign' 'Sports' 'Metro' 'Business' nan 'Culture' 'NODESK'\n",
      " 'Science' 'Upshot' 'Politics' 'Editorial' 'Universal' 'Weekend'\n",
      " 'Magazine' 'OpEd' 'Styles' 'Investigative' 'Summary' 'Climate' 'Obits'\n",
      " 'Washington' 'Dining' 'BookReview' 'Express' 'Books' 'NYTI' 'Travel'\n",
      " 'SundayBusiness' 'RealEstate' 'Well' 'NYTNow' 'Projects and Initiatives'\n",
      " 'Headway' 'Local Investigations' 'Video']\n"
     ]
    }
   ],
   "source": [
    "#print unique values for news desk\n",
    "print(rest[\"newsdesk\"].unique())\n",
    "print(front_pages[\"newsdesk\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sports' 'Blogs' 'Home & Garden' 'Great Homes & Destinations' 'Opinion'\n",
      " 'Business Day' 'Crosswords & Games' 'U.S.' 'Health' 'Arts' 'New York'\n",
      " 'Magazine' 'World' 'Technology' 'Style' 'Books' 'T Magazine' 'Theater'\n",
      " 'Travel' 'Multimedia/Photos' 'Education' 'Science' 'Automobiles'\n",
      " 'Fashion & Style' 'Food' 'Your Money' 'Movies' 'Real Estate' 'Job Market'\n",
      " 'Sunday Review' 'Booming' 'Times Topics' 'Public Editor' 'Corrections'\n",
      " 'Archives' 'International Home' 'The Upshot' 'Times Insider' 'Giving'\n",
      " 'Obituaries' 'NYT Now' 'Universal' 'Topics' 'Today‚Äôs Paper'\n",
      " 'membercenter' 'Podcasts' 'Briefing' 'Well' 'The Learning Network'\n",
      " 'Watching' 'Admin' 'Smarter Living' 'Climate' 'Reader Center' 'Lens'\n",
      " 'Homepage' 'Neediest Cases' 'Parenting' 'The Weekly' 'Video' 'Guides'\n",
      " 'At Home' 'en Espa√±ol' 'Special Series' 'The New York Times Presents' nan\n",
      " 'Headway' 'Book Review' 'Guide' 'Weather']\n",
      "['U.S.' 'World' 'Sports' 'New York' 'Health' 'Business Day' 'Technology'\n",
      " 'Arts' 'Your Money' 'Science' 'Movies' 'Education' 'Books' 'Theater'\n",
      " 'The Upshot' 'Food' 'Obituaries' 'Style' 'Universal' 'Travel' 'Opinion'\n",
      " 'Magazine' 'Fashion & Style' 'Real Estate' 'Well' 'Climate'\n",
      " 'Today‚Äôs Paper' 'Briefing' 'International Home' 'Headway' 'Corrections']\n"
     ]
    }
   ],
   "source": [
    "#print unique values for section name\n",
    "print(rest[\"sectionname\"].unique())\n",
    "print(front_pages[\"sectionname\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['News' 'Question' 'Letter' 'Obituary (Obit)' 'Schedule' 'Review'\n",
      " 'Interview' 'Text' 'An Analysis' 'List' 'Op-Ed' 'Brief' 'News Analysis'\n",
      " 'Editorial' 'Summary' 'Addendum' 'Correction' 'recipe' 'Caption'\n",
      " 'Special Report' 'An Appraisal' 'Quote' 'First Chapter' 'Biography'\n",
      " 'Series' 'briefing' 'Newsletter' \"Editors' Note\"]\n",
      "['News' 'News Analysis' 'Obituary (Obit)' 'Biography' 'An Appraisal'\n",
      " 'Military Analysis' 'Review' 'Summary' 'Editorial' 'List' 'Op-Ed' 'Quote'\n",
      " 'Series' 'briefing']\n"
     ]
    }
   ],
   "source": [
    "#print unique values for type of material\n",
    "print(rest[\"typeofmaterial\"].unique())\n",
    "print(front_pages[\"typeofmaterial\"].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
